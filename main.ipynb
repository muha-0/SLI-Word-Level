{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_load_path = \"C:/projects/SLT_project_MP_images/model.keras\"\n",
    "\n",
    "# Load the model\n",
    "lstm_model = load_model(model_load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multistream_features(frame_sequence):\n",
    "    pose = frame_sequence[:, 21:54, :]       # Pose (33 landmarks)\n",
    "    left_hand = frame_sequence[:, :21, :]    # Left hand (21 landmarks)\n",
    "    right_hand = frame_sequence[:, 54:, :]   # Right hand (21 landmarks)\n",
    "    \n",
    "    return pose, left_hand, right_hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target glosses (words)\n",
    "target_glosses = ['any', 'thank you', 'bye', 'question']\n",
    "\n",
    "# Create bidirectional mappings (label to gloss & gloss to label)\n",
    "folder_to_label = {folder: idx for idx, folder in enumerate(target_glosses)}\n",
    "label_to_folder = {idx: folder for folder, idx in folder_to_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def extract_landmarks(image, hands, pose):\n",
    "    # Initialize landmarks with zeros\n",
    "    left_hand_landmarks = np.zeros((21, 3))\n",
    "    right_hand_landmarks = np.zeros((21, 3))\n",
    "    pose_landmarks = np.zeros((33, 3))\n",
    "    \n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5) as hands, \\\n",
    "         mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5) as pose:\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process hands and pose\n",
    "        hand_results = hands.process(image_rgb)\n",
    "        pose_results = pose.process(image_rgb)\n",
    "\n",
    "        # Extract hand landmarks\n",
    "        if hand_results.multi_hand_landmarks:\n",
    "            for idx, hand_landmarks_set in enumerate(hand_results.multi_hand_landmarks):\n",
    "                extracted = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks_set.landmark])\n",
    "                if hand_results.multi_handedness[idx].classification[0].label == 'Left':\n",
    "                    left_hand_landmarks = extracted\n",
    "                else:\n",
    "                    right_hand_landmarks = extracted\n",
    "\n",
    "        # Extract pose landmarks\n",
    "        if pose_results.pose_landmarks:\n",
    "            pose_landmarks = np.array([[lm.x, lm.y, lm.z] for lm in pose_results.pose_landmarks.landmark])\n",
    "        \n",
    "        # Combine all landmarks\n",
    "        all_landmarks = np.vstack([left_hand_landmarks, pose_landmarks, right_hand_landmarks])\n",
    "        print(all_landmarks)\n",
    "        hand_count = len(hand_results.multi_hand_landmarks) if hand_results.multi_hand_landmarks else 0\n",
    "        pose_detected = 1 if pose_results.pose_landmarks else 0\n",
    "        \n",
    "        print(all_landmarks.shape)\n",
    "        return all_landmarks, hand_count, pose_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30\n",
    "confidence_threshold = 0.90\n",
    "sliding_window = []\n",
    "\n",
    "def predict_real_time(frame):\n",
    "    global sliding_window\n",
    "    \n",
    "    # Extract landmarks\n",
    "    landmarks = extract_landmarks(frame)\n",
    "    sliding_window.append(landmarks)\n",
    "    \n",
    "    # Maintain window size\n",
    "    if len(sliding_window) > window_size:\n",
    "        sliding_window.pop(0)\n",
    "\n",
    "    # Only predict when enough frames are collected\n",
    "    if len(sliding_window) == window_size:\n",
    "        pose, left, right = process_multistream_features(np.array(sliding_window))\n",
    "        predictions = lstm_model.predict([pose[np.newaxis, :], left[np.newaxis, :], right[np.newaxis, :]])\n",
    "        \n",
    "        # Get the predicted class and confidence\n",
    "        predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "        confidence = np.max(predictions)\n",
    "\n",
    "        if confidence > confidence_threshold:\n",
    "            print(f\"Prediction: {label_to_folder[predicted_class]}, Confidence: {confidence:.2f}\")\n",
    "            return label_to_folder[predicted_class]\n",
    "\n",
    "    return \"No Prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # Webcam feed\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Predict in real-time\n",
    "    prediction = predict_real_time(frame)\n",
    "\n",
    "    # Display predictions on the frame\n",
    "    cv2.putText(frame, f\"Prediction: {prediction}\", (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Sign Language Detection\", frame)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
